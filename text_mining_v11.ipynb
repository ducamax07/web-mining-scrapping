{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appuie pour faire tourner les fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modules à installer se trouvent dans requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import nltk \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "STOP_WORDS.update(['employee', 'organization', 'work', 'job', 'company', \"'s\"])\n",
    "STEMMER = nltk.stem.SnowballStemmer('english')\n",
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du fichier de contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content_path, links_path):\n",
    "    if not os.path.exists(content_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{content_path}' est introuvable.\")\n",
    "    with open(content_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    if not os.path.exists(links_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{links_path}' est introuvable.\")\n",
    "    with open(links_path, 'r', encoding='utf-8') as file:\n",
    "        links = json.load(file)\n",
    "    \n",
    "    return content, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "def save_to_file(data, filename):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        np.savetxt(f'output/{filename}.txt', data, fmt='%.4f')\n",
    "    elif isinstance(data, dict):\n",
    "        with open(f'output/{filename}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    elif isinstance(data, list):\n",
    "        with open(f'output/{filename}.txt', 'w', encoding='utf-8') as f:\n",
    "            for line in data:\n",
    "                f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON TO GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_gml (links_path, output_file):\n",
    "    with open(links_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        links_path = json.load(file)\n",
    "    nodes = set()\n",
    "    edges = []\n",
    "    # Parcourir les liens et extraire les nœuds et les connexions\n",
    "    for source, targets in links_path.items():\n",
    "        nodes.add(source)\n",
    "        for target in targets:\n",
    "            nodes.add(target)\n",
    "            edges.append((source, target))\n",
    "\n",
    "    # Étape 3 : Assigner des ID aux nœuds (sans utiliser enumerate)\n",
    "    node_id = {}\n",
    "    current_id = 0\n",
    "    for node in nodes:\n",
    "        node_id[node] = current_id\n",
    "        current_id += 1\n",
    "\n",
    "    # Étape 4 : Écrire le fichier GML\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"graph\\n\")\n",
    "        file.write(\"[\\n\")\n",
    "        file.write(\"  directed 1\\n\")  # Graph orienté\n",
    "\n",
    "        # Ajouter les nœuds\n",
    "        for node, id in node_id.items():\n",
    "            file.write(\"  node\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    id {id}\\n\")\n",
    "            file.write(f\"    label \\\"{node}\\\"\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        # Ajouter les arêtes\n",
    "        for source, target in edges:\n",
    "            file.write(\"  edge\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    source {node_id[source]}\\n\")\n",
    "            file.write(f\"    target {node_id[target]}\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        file.write(\"]\\n\")\n",
    "\n",
    "    print(f\"Fichier GML créé : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement du contenu texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [STEMMER.stem(token) for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder tous les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_tokens(content, output_file):\n",
    "    cleaned_data = {}\n",
    "\n",
    "    for page_title, page_content in content.items():\n",
    "        tokens = preprocess_text(page_content)\n",
    "        cleaned_data[page_title] = tokens\n",
    "\n",
    "    save_to_file(cleaned_data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement de texte spécifique à SIA, pour éviter de supprimer les \"not\" et autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_SIA(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    all_tokens = []\n",
    "    for content in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(content))\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différents modèles de text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouver les phrases comprenant un ou des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_with_words(corpus, words):\n",
    "    sentences_with_words = []\n",
    "    for content in corpus.values():\n",
    "        sentences = sent_tokenize(content)\n",
    "        for sentence in sentences:\n",
    "            if all(word in sentence.lower() for word in words):\n",
    "                sentences_with_words.append(sentence)\n",
    "    return sentences_with_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_sentiment(sentences):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"compound\": 0}\n",
    "    for sentence in sentences:\n",
    "        scores = sia.polarity_scores(sentence)\n",
    "        sentiment_scores[\"positive\"] += scores[\"pos\"]\n",
    "        sentiment_scores[\"negative\"] += scores[\"neg\"]\n",
    "        sentiment_scores[\"neutral\"] += scores[\"neu\"]\n",
    "        sentiment_scores[\"compound\"] += scores[\"compound\"]\n",
    "\n",
    "    # Moyenne des scores\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences > 0:\n",
    "        sentiment_scores = {key: value / num_sentences for key, value in sentiment_scores.items()}\n",
    "\n",
    "    # Déterminer le sentiment principal\n",
    "    if sentiment_scores[\"positive\"] > sentiment_scores[\"negative\"] and sentiment_scores[\"positive\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment = \"Positif\"\n",
    "    elif sentiment_scores[\"negative\"] > sentiment_scores[\"positive\"] and sentiment_scores[\"negative\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment = \"Négatif\"\n",
    "    else:\n",
    "        sentiment = \"Neutre\"\n",
    "\n",
    "    return sentiment_scores, sentiment\n",
    "\n",
    "def word_sentiment_analysis(content, word):\n",
    "\n",
    "    # Trouver les phrases contenant le mot\n",
    "    sentences_with_word = find_sentences_with_words(content, word)\n",
    "    if not sentences_with_word:\n",
    "        return f\"Le mot '{word}' n'apparaît pas dans le corpus.\"\n",
    "\n",
    "    # Analyser le sentiment des phrases\n",
    "    sentiment_scores, sentiment = analyze_word_sentiment(sentences_with_word)\n",
    "\n",
    "    return {\n",
    "        \"word\": word,\n",
    "        \"sentences_with_word\": sentences_with_word,\n",
    "        \"sentiment_scores\": sentiment_scores,\n",
    "        \"overall_sentiment\": sentiment\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooccurrences pour le mot-clé 'diversity':\n",
      "\n",
      "Cooccurrence        \tFrequency \tMean Distance  \n",
      "------------------------------------------------------------\n",
      "cultur divers           \t232       \t1.34           \n",
      "divers inclus           \t192       \t1.84           \n",
      "divers equiti           \t134       \t1.54           \n",
      "divers divers           \t116       \t3.41           \n",
      "divers train            \t102       \t1.30           \n",
      "divers cultur           \t83        \t2.41           \n",
      "promot divers           \t61        \t1.97           \n",
      "divers manag            \t51        \t2.06           \n",
      "divers statement        \t48        \t1.08           \n",
      "divers hire             \t47        \t2.14           \n"
     ]
    }
   ],
   "source": [
    "def find_cooccurrences(corpus_path, keyword, min_freq=2, window_size=5):\n",
    "    # Charger et prétraiter le corpus\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = json.load(file)\n",
    "    \n",
    "    all_tokens = []\n",
    "    for text in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(text))\n",
    "    \n",
    "    # Trouver les bigrammes avec la librairie NLTK\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(all_tokens, window_size=window_size)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    \n",
    "    # Filtrer les bigrammes contenant le mot-clé\n",
    "    keyword_stem = STEMMER.stem(keyword)\n",
    "    relevant_bigrams = {}\n",
    "    for bigram, freq in finder.ngram_fd.items():\n",
    "        if keyword_stem in bigram:\n",
    "            distances = []\n",
    "            for i in range(len(all_tokens) - 1):\n",
    "                if all_tokens[i] == bigram[0] and bigram[1] in all_tokens[i + 1:i + window_size + 1]:\n",
    "                    j = all_tokens.index(bigram[1], i + 1, i + window_size + 1)\n",
    "                    distances.append(abs(j - i))\n",
    "            relevant_bigrams[bigram] = {\n",
    "                \"frequency\": freq,\n",
    "                \"mean_distance\": sum(distances) / len(distances) if distances else 0\n",
    "            }\n",
    "    \n",
    "    # Trier les résultats\n",
    "    sorted_bigrams = sorted(relevant_bigrams.items(), key=lambda item: item[1]['frequency'], reverse=True)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"Cooccurrences pour le mot-clé '{keyword}':\\n\")\n",
    "    print(f\"{'Cooccurrence':<20}\\t{'Frequency':<10}\\t{'Mean Distance':<15}\")\n",
    "    print(f\"{'-' * 60}\")\n",
    "    for bigram, data in sorted_bigrams:\n",
    "        print(f\"{bigram[0]} {bigram[1]:<17}\\t{data['frequency']:<10}\\t{data['mean_distance']:<15.2f}\")\n",
    "    \n",
    "    return sorted_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_tokens(corpus, top_n=20):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    return freq_dist.most_common(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(corpus, output_filename='wordcloud.png'):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    text = ' '.join(all_tokens)\n",
    "    wordcloud = WordCloud(background_color='white', stopwords=STOP_WORDS, max_words=30, min_font_size=10).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'output/{output_filename}', format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tableau des tops tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_table(json_path, output_csv):\n",
    "    STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "    STEMMER = SnowballStemmer('english')\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    token_counts = {}  # Dictionnaire pour les occurrences\n",
    "    document_counts = defaultdict(int)  # Nombre de documents contenant chaque token\n",
    "    word_map = defaultdict(set)  # Mots originaux associés aux tokens stemmés\n",
    "\n",
    "\n",
    "    for text in content.values():\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stemmed_tokens = [STEMMER.stem(token) for token in tokens if token.isalnum() and token not in STOP_WORDS]\n",
    "        #Compter les occurrences globales\n",
    "        for token in stemmed_tokens:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "        #Compter les documents contenant chaque token (uniquement une fois par document)\n",
    "        unique_tokens = set(stemmed_tokens)\n",
    "        for token in unique_tokens:\n",
    "            document_counts[token] += 1\n",
    "\n",
    "        #Mapper les mots originaux aux tokens stemmés\n",
    "        for word in tokens:\n",
    "            if word.isalnum() and word not in STOP_WORDS:\n",
    "                stemmed = STEMMER.stem(word)\n",
    "                word_map[stemmed].add(word)\n",
    "\n",
    "    # Construire et sauvegarder le tableau en une étape\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Écrire l'en-tête\n",
    "        writer.writerow([\"Token (stemmatisé)\", \"Mots associés\", \"Occurrences\", \"Articles concernés\"])\n",
    "        \n",
    "        # Écrire les données\n",
    "        for token, freq in token_counts.items():\n",
    "            writer.writerow([\n",
    "                token,\n",
    "                ', '.join(word_map[token]),  # Convertir les mots associés en chaîne de caractères\n",
    "                freq,\n",
    "                document_counts[token]\n",
    "            ])\n",
    "    print(f\"Tableau sauvegardé dans {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice de similarité par cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_cosine_similarity(content):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(list(content.values()))\n",
    "    \n",
    "    # Calcul de la matrice de similarité par le cosinus\n",
    "    similarity_matrix = cosine_similarity(X_tfidf)\n",
    "    save_to_file(similarity_matrix, 'similarity_matrix')\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer le nombre optimal de clusters \n",
    "- par la méthode du coude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(content, max_k=10):\n",
    "    with open(content, encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(list(content.values()))\n",
    "    distortions = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, max_iter=100, n_init=5, random_state=42)\n",
    "        kmeans.fit(X_tfidf)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    # Crée le répertoire output s'il n'existe pas\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, max_k + 1), distortions, marker='o')\n",
    "    plt.xlabel('Nombre de Clusters')\n",
    "    plt.ylabel('Distorsion')\n",
    "    plt.title('Méthode du Coude')\n",
    "    plt.savefig('output/elbow_method.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Retourne le nombre optimal basé sur l'inflexion\n",
    "    optimal_k = np.diff(distortions).argmin() + 2\n",
    "    return optimal_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_documents(content, num_clusters):\n",
    "    with open(content, encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "\n",
    "    documents = list(content.values())\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_clusters, max_iter=100, n_init=10, random_state=42)\n",
    "    kmeans.fit(X_tfidf)\n",
    "    \n",
    "    clusters = {}\n",
    "    for i, doc in enumerate(documents):\n",
    "        cluster_label = int(kmeans.labels_[i])\n",
    "        document_key = list(content.keys())[i]  # Utilisation de la clé du dictionnaire\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = []\n",
    "        clusters[cluster_label].append(document_key)\n",
    "    \n",
    "    save_to_file(clusters, 'clusters.json')\n",
    "\n",
    "    # Calcul et sauvegarde de la matrice de similarité\n",
    "    similarity_matrix = calculate_cosine_similarity(content)\n",
    "    \n",
    "    return clusters, similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde les titres complets des documents dans chaque cluster dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clusters_individually(clusters, content_path):\n",
    "    # Charger le contenu à partir du fichier JSON\n",
    "    with open(content_path, 'r', encoding='utf-8') as f:\n",
    "        content = json.load(f)\n",
    "\n",
    "    # Créer un dossier pour stocker les fichiers des clusters\n",
    "    os.makedirs('output/clusters', exist_ok=True)\n",
    "    \n",
    "    # Sauvegarder chaque cluster dans un fichier JSON séparé\n",
    "    for cluster_label, docs in clusters.items():\n",
    "        cluster_content = {doc: content[doc] for doc in docs}  # Documents de ce cluster\n",
    "        output_file = f'output/clusters/cluster_{cluster_label}.json'  # Nom du fichier JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cluster_content, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Cluster {cluster_label} sauvegardé dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links par clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_links_per_cluster(clusters_dir, original_links_path, output_dir):\n",
    "    # Charger les liens originaux\n",
    "    with open(original_links_path, 'r', encoding='utf-8') as f:\n",
    "        original_links = json.load(f)\n",
    "\n",
    "    # Créer un répertoire pour stocker les nouveaux fichiers\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Parcourir tous les fichiers de clusters dans le répertoire spécifié\n",
    "    for cluster_file in os.listdir(clusters_dir):\n",
    "        cluster_path = os.path.join(clusters_dir, cluster_file)\n",
    "\n",
    "        # Charger le fichier du cluster\n",
    "        with open(cluster_path, 'r', encoding='utf-8') as f:\n",
    "            cluster_nodes = json.load(f)\n",
    "\n",
    "        # Préparer un dictionnaire pour les liens de ce cluster\n",
    "        cluster_links = {}\n",
    "\n",
    "        # Parcourir les nœuds du cluster et ajouter leurs liens\n",
    "        for node in cluster_nodes:\n",
    "            if node in original_links:\n",
    "                cluster_links[node] = [\n",
    "                    target for target in original_links[node] if target in cluster_nodes\n",
    "                ]\n",
    "\n",
    "        # Nommer le fichier de sortie basé sur le fichier du cluster\n",
    "        cluster_label = os.path.splitext(cluster_file)[0]\n",
    "        output_path = os.path.join(output_dir, f\"{cluster_label}_links.json\")\n",
    "\n",
    "        # Sauvegarder les liens filtrés dans un fichier JSON\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cluster_links, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Fichier de liens créé pour le cluster : {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph de similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarity_graph(content, similarity_matrix, output_file):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Ajouter les nœuds\n",
    "    for page in content.keys():\n",
    "        G.add_node(page)\n",
    "\n",
    "    # Ajouter les arêtes pondérées par les similarités textuelles\n",
    "    pages = list(content.keys())\n",
    "    for i in range(len(pages)):\n",
    "        for j in range(i + 1, len(pages)):\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            if similarity > 0:  # Ajouter une arête seulement si la similarité est positive\n",
    "                G.add_edge(pages[i], pages[j], weight=similarity)\n",
    "\n",
    "    # Sauvegarder le graphe en format GML\n",
    "    nx.write_gml(G, output_file)\n",
    "    print(f\"Graphe de similarité créé : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisis ce que tu veux lancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chemin du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    content_path = 'content3.json'\n",
    "    links_path = 'links3.json'\n",
    "    content, links = load_data(content_path, links_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases content un/des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_find = ['profit', 'growth']\n",
    "sentences = find_sentences_with_words(content, words_to_find)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"profit\"  \n",
    "result = word_sentiment_analysis(content, word)\n",
    "\n",
    "print(f\"Analyse de sentiment pour le mot '{result['word']}':\")\n",
    "print(f\"Scores moyens : {result['sentiment_scores']}\")\n",
    "print(f\"Sentiment global : {result['overall_sentiment']}\")\n",
    "print(\"\\nExemples de phrases contenant le mot :\")\n",
    "for sentence in result[\"sentences_with_word\"][:5]:  # Afficher jusqu'à 5 phrases\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'cleaned_data'\n",
    "clean_and_save_tokens(content, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooccurrences pour le mot-clé 'diversity':\n",
      "\n",
      "Cooccurrence        \tFrequency \tMean Distance  \n",
      "------------------------------------------------------------\n",
      "cultur divers           \t232       \t1.34           \n",
      "divers inclus           \t192       \t1.84           \n",
      "divers equiti           \t134       \t1.54           \n",
      "divers divers           \t116       \t3.41           \n",
      "divers train            \t102       \t1.30           \n",
      "divers cultur           \t83        \t2.41           \n",
      "promot divers           \t61        \t1.97           \n",
      "divers manag            \t51        \t2.06           \n",
      "divers statement        \t48        \t1.08           \n",
      "divers hire             \t47        \t2.14           \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('cultur', 'divers'),\n",
       "  {'frequency': 232, 'mean_distance': 1.339366515837104}),\n",
       " (('divers', 'inclus'),\n",
       "  {'frequency': 192, 'mean_distance': 1.8402061855670102}),\n",
       " (('divers', 'equiti'),\n",
       "  {'frequency': 134, 'mean_distance': 1.5416666666666667}),\n",
       " (('divers', 'divers'),\n",
       "  {'frequency': 116, 'mean_distance': 3.411392405063291}),\n",
       " (('divers', 'train'),\n",
       "  {'frequency': 102, 'mean_distance': 1.3010752688172043}),\n",
       " (('divers', 'cultur'), {'frequency': 83, 'mean_distance': 2.409090909090909}),\n",
       " (('promot', 'divers'),\n",
       "  {'frequency': 61, 'mean_distance': 1.9672131147540983}),\n",
       " (('divers', 'manag'), {'frequency': 51, 'mean_distance': 2.056603773584906}),\n",
       " (('divers', 'statement'),\n",
       "  {'frequency': 48, 'mean_distance': 1.0833333333333333}),\n",
       " (('divers', 'hire'), {'frequency': 47, 'mean_distance': 2.142857142857143})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_path = 'content3.json'  # Remplacez par votre fichier JSON\n",
    "keyword = \"diversity\"         # Mot-clé à analyser\n",
    "find_cooccurrences(corpus_path, keyword, min_freq=47, window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = show_top_tokens(content)\n",
    "print(\"Top Tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Tokens Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'output/token_table.csv'\n",
    "generate_token_table(content_path, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = find_optimal_clusters(\"content3.json\", max_k=12)  \n",
    "print(f'Nombre optimal de clusters : {num_clusters}')\n",
    "\n",
    "clusters, similarity_matrix = cluster_documents(\"content3.json\", num_clusters)\n",
    "\n",
    "# Afficher et sauvegarder les contenus des clusters\n",
    "save_clusters_individually(clusters, \"content3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dir = \"output/clusters\"\n",
    "original_links_path = \"links3.json\"\n",
    "output_dir = \"cluster_links\"\n",
    "\n",
    "create_links_per_cluster(clusters_dir, original_links_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON to GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = \"links3.json\"\n",
    "output_file = \"outputgml/graph.gml\"\n",
    "json_to_gml(links, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file1 = \"outputgml/tokens_graph.gml\"\n",
    "output_file2 = \"outputgml/clusters_graph.gml\"\n",
    "tokens_file = \"output/cleaned_data.json\"\n",
    "clusters_file = \"output/clusters.json\"\n",
    "json_to_gml(tokens_file, output_file1)\n",
    "json_to_gml(clusters_file, output_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"outputgml/similarity_graph.gml\"\n",
    "generate_similarity_graph(content, similarity_matrix, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = find_optimal_clusters(\"content3.json\", max_k=10)\n",
    "for i in range(0,num_clusters):\n",
    "    json_to_gml(f\"cluster_links/cluster_{i}_links.json\", f\"outputgml/cluster_{i}.gml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
