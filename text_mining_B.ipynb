{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appuie pour faire tourner les fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modules à installer se trouvent dans requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import nltk \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "STEMMER = nltk.stem.SnowballStemmer('english')\n",
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de liens uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_links(links_path):\n",
    "    with open(links_path, 'r', encoding='utf-8') as file:\n",
    "        links = json.load(file)\n",
    "    \n",
    "    unique_links = set()\n",
    "    for targets in links.values():\n",
    "        unique_links.update(targets)\n",
    "    \n",
    "    return len(unique_links)\n",
    "\n",
    "# Example usage\n",
    "links_path = 'linksB.json'\n",
    "print(f\"Nombre de liens uniques enregistrés : {count_unique_links(links_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du fichier de contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content_path, links_path):\n",
    "    if not os.path.exists(content_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{content_path}' est introuvable.\")\n",
    "    with open(content_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    if not os.path.exists(links_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{links_path}' est introuvable.\")\n",
    "    with open(links_path, 'r', encoding='utf-8') as file:\n",
    "        links = json.load(file)\n",
    "    \n",
    "    return content, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "def save_to_file(data, filename):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        np.savetxt(f'output/{filename}.txt', data, fmt='%.4f')\n",
    "    elif isinstance(data, dict):\n",
    "        with open(f'output/{filename}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    elif isinstance(data, list):\n",
    "        with open(f'output/{filename}.txt', 'w', encoding='utf-8') as f:\n",
    "            for line in data:\n",
    "                f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON TO GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_gml (links_path, output_file):\n",
    "    with open(links_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        links_path = json.load(file)\n",
    "    nodes = set()\n",
    "    edges = []\n",
    "    # Parcourir les liens et extraire les nœuds et les connexions\n",
    "    for source, targets in links_path.items():\n",
    "        nodes.add(source)\n",
    "        for target in targets:\n",
    "            nodes.add(target)\n",
    "            edges.append((source, target))\n",
    "\n",
    "    # Étape 3 : Assigner des ID aux nœuds (sans utiliser enumerate)\n",
    "    node_id = {}\n",
    "    current_id = 0\n",
    "    for node in nodes:\n",
    "        node_id[node] = current_id\n",
    "        current_id += 1\n",
    "\n",
    "    # Étape 4 : Écrire le fichier GML\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"graph\\n\")\n",
    "        file.write(\"[\\n\")\n",
    "        file.write(\"  directed 1\\n\")  # Graph orienté\n",
    "\n",
    "        # Ajouter les nœuds\n",
    "        for node, id in node_id.items():\n",
    "            file.write(\"  node\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    id {id}\\n\")\n",
    "            file.write(f\"    label \\\"{node}\\\"\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        # Ajouter les arêtes\n",
    "        for source, target in edges:\n",
    "            file.write(\"  edge\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    source {node_id[source]}\\n\")\n",
    "            file.write(f\"    target {node_id[target]}\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        file.write(\"]\\n\")\n",
    "\n",
    "    print(f\"Fichier GML créé : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement du contenu texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [STEMMER.stem(token) for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder tous les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_tokens(content, output_file):\n",
    "    cleaned_data = {}\n",
    "\n",
    "    for page_title, page_content in content.items():\n",
    "        tokens = preprocess_text(page_content)\n",
    "        cleaned_data[page_title] = tokens\n",
    "\n",
    "    save_to_file(cleaned_data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement de texte spécifique à SIA, pour éviter de supprimer les \"not\" et autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_SIA(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    all_tokens = []\n",
    "    for content in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(content))\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement des modèles tfidfj et doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Entraînement du modèle Doc2Vec\n",
    "def train_doc2vec_model(documents, vector_size=100, window=1, epochs=20):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "    model = Doc2Vec(tagged_data, vector_size=vector_size, window=2, min_count=1, workers=4, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "# 2. Entraînement du modèle TF-IDF\n",
    "def train_tfidf_model(documents):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différents modèles de text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouver les phrases comprenant un ou des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_with_words(corpus, words):\n",
    "    sentences_with_words = []\n",
    "    \n",
    "    # Normaliser les mots recherchés (racines)\n",
    "    stemmed_words = [STEMMER.stem(word.lower()) for word in words]\n",
    "    \n",
    "    for content in corpus.values():\n",
    "        # Découper le contenu en phrases\n",
    "        sentences = sent_tokenize(content)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Tokeniser la phrase et normaliser les tokens\n",
    "            tokens = word_tokenize(sentence.lower())\n",
    "            stemmed_tokens = [STEMMER.stem(token) for token in tokens]\n",
    "            # Vérifier si toutes les racines des mots recherchés sont présentes\n",
    "            if all(word in stemmed_tokens for word in stemmed_words):\n",
    "                sentences_with_words.append(sentence)\n",
    "    \n",
    "    return sentences_with_words\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_sentiment(sentences):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"compound\": 0}\n",
    "    for sentence in sentences:\n",
    "        scores = sia.polarity_scores(sentence)\n",
    "        sentiment_scores[\"positive\"] += scores[\"pos\"]\n",
    "        sentiment_scores[\"negative\"] += scores[\"neg\"]\n",
    "        sentiment_scores[\"neutral\"] += scores[\"neu\"]\n",
    "        sentiment_scores[\"compound\"] += scores[\"compound\"]\n",
    "\n",
    "    # Moyenne des scores\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences > 0:\n",
    "        sentiment_scores = {key: value / num_sentences for key, value in sentiment_scores.items()}\n",
    "\n",
    "    # Déterminer le sentiment principal\n",
    "    if sentiment_scores[\"positive\"] > sentiment_scores[\"negative\"] and sentiment_scores[\"positive\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment = \"Positif\"\n",
    "    elif sentiment_scores[\"negative\"] > sentiment_scores[\"positive\"] and sentiment_scores[\"negative\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment = \"Négatif\"\n",
    "    else:\n",
    "        sentiment = \"Neutre\"\n",
    "\n",
    "    return sentiment_scores, sentiment\n",
    "\n",
    "def word_sentiment_analysis(content, word):\n",
    "    print(word)\n",
    "    # Trouver les phrases contenant le mot\n",
    "    sentences_with_word = find_sentences_with_words(content, word)\n",
    "    print(sentences_with_word)\n",
    "    if not sentences_with_word:\n",
    "        print(f\"Le mot '{word}' n'a pas été trouvé dans le corpus.\")\n",
    "        return {\n",
    "            \"word\": word,\n",
    "            \"sentences_with_word\": [],\n",
    "            \"sentiment_scores\": {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"compound\": 0},\n",
    "            \"overall_sentiment\": \"Aucun (mot non trouvé)\"\n",
    "        }\n",
    "\n",
    "    # Analyser le sentiment des phrases\n",
    "    sentiment_scores, sentiment = analyze_word_sentiment(sentences_with_word)\n",
    "\n",
    "    return {\n",
    "        \"word\": word,\n",
    "        \"sentences_with_word\": sentences_with_word,\n",
    "        \"sentiment_scores\": sentiment_scores,\n",
    "        \"overall_sentiment\": sentiment\n",
    "    }\n",
    "\n",
    "def analyze_page_sentiment(page_name, content):\n",
    "    if page_name not in content:\n",
    "        return f\"La page '{page_name}' n'existe pas dans le corpus.\"\n",
    "\n",
    "    page_content = content[page_name]\n",
    "    sentences = sent_tokenize(page_content)\n",
    "    sentiment_scores, overall_sentiment = analyze_word_sentiment(sentences)\n",
    "\n",
    "    # Ajout pour cas neutre avec consonance\n",
    "    if overall_sentiment == \"Neutre\":\n",
    "        if sentiment_scores[\"positive\"] > sentiment_scores[\"negative\"]:\n",
    "            overall_sentiment = \"Neutre avec consonance plus positive\"\n",
    "        elif sentiment_scores[\"negative\"] > sentiment_scores[\"positive\"]:\n",
    "            overall_sentiment = \"Neutre avec consonance plus négative\"\n",
    "\n",
    "    return {\n",
    "        \"page_name\": page_name,\n",
    "        \"sentiment_scores\": sentiment_scores,\n",
    "        \"overall_sentiment\": overall_sentiment\n",
    "    }\n",
    "\n",
    "def analyze_corpus_sentiment(corpus):\n",
    "    overall_sentiment_scores = {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"compound\": 0}\n",
    "    document_sentiments = {}\n",
    "\n",
    "    for doc_name, doc_content in corpus.items():\n",
    "        sentences = sent_tokenize(doc_content)\n",
    "        sentiment_scores, overall_sentiment = analyze_word_sentiment(sentences)\n",
    "        document_sentiments[doc_name] = {\n",
    "            \"sentiment_scores\": sentiment_scores,\n",
    "            \"overall_sentiment\": overall_sentiment\n",
    "        }\n",
    "\n",
    "        # Accumuler les scores de sentiment pour l'ensemble du corpus\n",
    "        for key in overall_sentiment_scores:\n",
    "            overall_sentiment_scores[key] += sentiment_scores[key]\n",
    "\n",
    "        # Moyenne des scores pour l'ensemble du corpus\n",
    "    num_documents = len(corpus)\n",
    "    if num_documents > 0:\n",
    "        overall_sentiment_scores = {key: value / num_documents for key, value in overall_sentiment_scores.items()}\n",
    "\n",
    "    return {\n",
    "        \"overall_sentiment_scores\": overall_sentiment_scores,\n",
    "        \"document_sentiments\": document_sentiments\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cooccurrences(corpus_path, keyword, min_freq=2, window_size=5):\n",
    "    # Charger et prétraiter le corpus\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = json.load(file)\n",
    "    \n",
    "    all_tokens = []\n",
    "    for text in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(text))\n",
    "    \n",
    "    # Trouver les bigrammes avec la librairie NLTK\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(all_tokens, window_size=window_size)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    \n",
    "    # Filtrer les bigrammes contenant le mot-clé\n",
    "    keyword_stem = STEMMER.stem(keyword)\n",
    "    relevant_bigrams = {}\n",
    "    for bigram, freq in finder.ngram_fd.items():\n",
    "        if keyword_stem in bigram:\n",
    "            distances = []\n",
    "            for i in range(len(all_tokens) - 1):\n",
    "                if all_tokens[i] == bigram[0] and bigram[1] in all_tokens[i + 1:i + window_size + 1]:\n",
    "                    j = all_tokens.index(bigram[1], i + 1, i + window_size + 1)\n",
    "                    distances.append(abs(j - i))\n",
    "            relevant_bigrams[bigram] = {\n",
    "                \"frequency\": freq,\n",
    "                \"mean_distance\": sum(distances) / len(distances) if distances else 0\n",
    "            }\n",
    "    \n",
    "    # Trier les résultats\n",
    "    sorted_bigrams = sorted(relevant_bigrams.items(), key=lambda item: item[1]['frequency'], reverse=True)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"Cooccurrences pour le mot-clé '{keyword}':\\n\")\n",
    "    print(f\"{'Cooccurrence':<20}\\t{'Frequency':<10}\\t{'Mean Distance':<15}\")\n",
    "    print(f\"{'-' * 60}\")\n",
    "    for bigram, data in sorted_bigrams:\n",
    "        print(f\"{bigram[0]} {bigram[1]:<17}\\t{data['frequency']:<10}\\t{data['mean_distance']:<15.2f}\")\n",
    "    \n",
    "    return sorted_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trigram_cooccurrences(corpus_path, keyword, min_freq=2, window_size=5):\n",
    "    # Charger et prétraiter le corpus\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = json.load(file)\n",
    "    \n",
    "    all_tokens = []\n",
    "    for text in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(text))\n",
    "    \n",
    "    # Trouver les trigrammes avec la librairie NLTK\n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "    finder = nltk.collocations.TrigramCollocationFinder.from_words(all_tokens, window_size=window_size)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    \n",
    "    # Filtrer les trigrammes contenant le mot-clé\n",
    "    keyword_stem = STEMMER.stem(keyword)\n",
    "    relevant_trigrams = {}\n",
    "    for trigram, freq in finder.ngram_fd.items():\n",
    "        if keyword_stem in trigram:\n",
    "            distances = []\n",
    "            for i in range(len(all_tokens) - 2):\n",
    "                if all_tokens[i] == trigram[0] and trigram[1] in all_tokens[i + 1:i + window_size + 1] and trigram[2] in all_tokens[i + 2:i + window_size + 2]:\n",
    "                    j = all_tokens.index(trigram[1], i + 1, i + window_size + 1)\n",
    "                    k = all_tokens.index(trigram[2], i + 2, i + window_size + 2)\n",
    "                    distances.append(abs(k - i))\n",
    "            relevant_trigrams[trigram] = {\n",
    "                \"frequency\": freq,\n",
    "                \"mean_distance\": sum(distances) / len(distances) if distances else 0\n",
    "            }\n",
    "    \n",
    "    # Trier les résultats\n",
    "    sorted_trigrams = sorted(relevant_trigrams.items(), key=lambda item: item[1]['frequency'], reverse=True)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"Cooccurrences pour le mot-clé '{keyword}':\\n\")\n",
    "    print(f\"{'Cooccurrence':<30}\\t{'Frequency':<10}\\t{'Mean Distance':<15}\")\n",
    "    print(f\"{'-' * 60}\")\n",
    "    for trigram, data in sorted_trigrams:\n",
    "        print(f\"{trigram[0]} {trigram[1]} {trigram[2]:<17}\\t{data['frequency']:<10}\\t{data['mean_distance']:<15.2f}\")\n",
    "    \n",
    "    return sorted_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_tokens(corpus, top_n=20):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    return freq_dist.most_common(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(corpus, output_filename='wordcloud.png'):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    text = ' '.join(all_tokens)\n",
    "    wordcloud = WordCloud(background_color='white', stopwords=STOP_WORDS, max_words=30, min_font_size=10).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'output/{output_filename}', format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tableau des tops tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_table(json_path, output_csv):\n",
    "    STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "    STEMMER = SnowballStemmer('english')\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    token_counts = {}  # Dictionnaire pour les occurrences\n",
    "    document_counts = defaultdict(int)  # Nombre de documents contenant chaque token\n",
    "    word_map = defaultdict(set)  # Mots originaux associés aux tokens stemmés\n",
    "\n",
    "\n",
    "    for text in content.values():\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stemmed_tokens = [STEMMER.stem(token) for token in tokens if token.isalnum() and token not in STOP_WORDS]\n",
    "        #Compter les occurrences globales\n",
    "        for token in stemmed_tokens:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "        #Compter les documents contenant chaque token (uniquement une fois par document)\n",
    "        unique_tokens = set(stemmed_tokens)\n",
    "        for token in unique_tokens:\n",
    "            document_counts[token] += 1\n",
    "\n",
    "        #Mapper les mots originaux aux tokens stemmés\n",
    "        for word in tokens:\n",
    "            if word.isalnum() and word not in STOP_WORDS:\n",
    "                stemmed = STEMMER.stem(word)\n",
    "                word_map[stemmed].add(word)\n",
    "\n",
    "    # Construire et sauvegarder le tableau en une étape\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Écrire l'en-tête\n",
    "        writer.writerow([\"Token (stemmatisé)\", \"Mots associés\", \"Occurrences\", \"Articles concernés\"])\n",
    "        \n",
    "        # Écrire les données\n",
    "        for token, freq in token_counts.items():\n",
    "            writer.writerow([\n",
    "                token,\n",
    "                ', '.join(word_map[token]),  # Convertir les mots associés en chaîne de caractères\n",
    "                freq,\n",
    "                document_counts[token]\n",
    "            ])\n",
    "    print(f\"Tableau sauvegardé dans {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de similarité entre 2 documents au choix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(model, doc_id1, doc_id2):\n",
    "    vec1 = model.dv[doc_id1]\n",
    "    vec2 = model.dv[doc_id2]\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche de documents similaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_docs_tfidf(content, tfidf_matrix, title, top_n=4):\n",
    "    # Vérifier si le titre existe dans le contenu\n",
    "    if title not in content:\n",
    "        raise ValueError(f\"Le titre '{title}' n'existe pas dans le contenu.\")\n",
    "    \n",
    "    # Obtenir l'index du document correspondant au titre\n",
    "    doc_index = list(content.keys()).index(title)\n",
    "    \n",
    "    # Calculer la similarité cosinus entre le document donné et tous les autres documents\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix[doc_index], tfidf_matrix).flatten()\n",
    "    \n",
    "    # Obtenir les indices des documents les plus similaires (en excluant le document lui-même)\n",
    "    similar_indices = cosine_similarities.argsort()[::-1][1:top_n+1]\n",
    "    \n",
    "    # Obtenir les titres des documents les plus similaires\n",
    "    similar_docs = [(list(content.keys())[i], cosine_similarities[i]) for i in similar_indices]\n",
    "    # Ajouter le document de base et les documents similaires dans un fichier de sortie\n",
    "    output_file = os.path.join(f\"outputidm/{title}.txt\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        # Ajouter le document de base\n",
    "        file.write(f\"Document de base: {title}\\n\")\n",
    "        file.write(content[title] + \"\\n\\n\")\n",
    "        \n",
    "        # Ajouter les documents similaires\n",
    "        for doc_title, similarity in similar_docs:\n",
    "            file.write(f\"Document similaire: {doc_title} (Similarité: {similarity:.4f})\\n\")\n",
    "            file.write(content[doc_title] + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Documents sauvegardés dans {output_file}\")\n",
    "    return similar_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice de similarité par cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(content, model):\n",
    "    # Obtenir les vecteurs des documents\n",
    "    doc_vectors = np.array([model.dv[i] for i in range(len(content))])  # Convertir en matrice NumPy\n",
    "\n",
    "    # Calcul de la matrice de similarité par le cosinus\n",
    "    similarity_matrix = cosine_similarity(doc_vectors)\n",
    "\n",
    "    # Sauvegarder la matrice de similarité dans un fichier\n",
    "    save_to_file(similarity_matrix, 'similarity_matrix.json')  # Sauvegarde en JSON\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre optimal de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(vectors, max_clusters=10, min_clusters=2):\n",
    "    distortions = []\n",
    "    for k in range(min_clusters, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(vectors)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(min_clusters, max_clusters + 1), distortions, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the elbow point\n",
    "    optimal_k = min_clusters\n",
    "    for i in range(1, len(distortions) - 1):\n",
    "        if distortions[i] - distortions[i + 1] < distortions[i - 1] - distortions[i]:\n",
    "            optimal_k = min_clusters + i\n",
    "            break\n",
    "\n",
    "    print(f\"Nombre optimal de clusters : {optimal_k}\")\n",
    "    return optimal_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_documents(content, optimal_k):\n",
    "\n",
    "    # Préparer les documents\n",
    "    documents = list(content.values())\n",
    "\n",
    "    # Entraîner le modèle Doc2Vec\n",
    "    model = train_doc2vec_model(documents, vector_size=100, window=2, epochs=40)\n",
    "\n",
    "    # Obtenir les vecteurs des documents\n",
    "    doc_vectors = [model.dv[i] for i in range(len(content))]\n",
    "\n",
    "    # Appliquer KMeans\n",
    "    kmeans = KMeans(n_clusters=optimal_k, max_iter=100, n_init=10, random_state=42)\n",
    "    kmeans.fit(doc_vectors)\n",
    "\n",
    "    clusters = {}\n",
    "    for i, doc in enumerate(doc_vectors):\n",
    "        cluster_label = int(kmeans.labels_[i])\n",
    "        document_key = list(content.keys())[i]  # Utilisation de la clé du dictionnaire\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = []\n",
    "        clusters[cluster_label].append(document_key)\n",
    "\n",
    "    # Sauvegarder les clusters\n",
    "    save_to_file(clusters, 'clusters')\n",
    "    \n",
    "    # Créer un fichier texte avec les titres des documents regroupés par clusters\n",
    "    with open('output/clusters_titles.txt', 'w', encoding='utf-8') as f:\n",
    "        for cluster_label, docs in clusters.items():\n",
    "            f.write(f\"Cluster {cluster_label}:\\n\")\n",
    "            for doc in docs:\n",
    "                f.write(f\"- {doc}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Calcul et sauvegarde de la matrice de similarité\n",
    "    similarity_matrix = calculate_cosine_similarity(content, model)\n",
    "\n",
    "    return clusters, similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde les titres complets des documents dans chaque cluster dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clusters_individually(clusters, content_path):\n",
    "    # Charger le contenu à partir du fichier JSON\n",
    "    with open(content_path, 'r', encoding='utf-8') as f:\n",
    "        content = json.load(f)\n",
    "\n",
    "    # Créer un dossier pour stocker les fichiers des clusters\n",
    "    os.makedirs('output/clusters', exist_ok=True)\n",
    "    \n",
    "    # Sauvegarder chaque cluster dans un fichier JSON séparé\n",
    "    for cluster_label, docs in clusters.items():\n",
    "        cluster_content = {doc: content[doc] for doc in docs}  # Documents de ce cluster\n",
    "        output_file = f'output/clusters/cluster_{cluster_label}.json'  # Nom du fichier JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cluster_content, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Cluster {cluster_label} sauvegardé dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links par clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_links_per_cluster(clusters_dir, original_links_path, output_dir):\n",
    "    # Charger les liens originaux\n",
    "    with open(original_links_path, 'r', encoding='utf-8') as f:\n",
    "        original_links = json.load(f)\n",
    "\n",
    "    # Créer un répertoire pour stocker les nouveaux fichiers\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Parcourir tous les fichiers de clusters dans le répertoire spécifié\n",
    "    for cluster_file in os.listdir(clusters_dir):\n",
    "        cluster_path = os.path.join(clusters_dir, cluster_file)\n",
    "\n",
    "        # Charger le fichier du cluster\n",
    "        with open(cluster_path, 'r', encoding='utf-8') as f:\n",
    "            cluster_nodes = json.load(f)\n",
    "\n",
    "        # Préparer un dictionnaire pour les liens de ce cluster\n",
    "        cluster_links = {}\n",
    "\n",
    "        # Parcourir les nœuds du cluster et ajouter leurs liens\n",
    "        for node in cluster_nodes:\n",
    "            if node in original_links:\n",
    "                cluster_links[node] = [\n",
    "                    target for target in original_links[node] if target in cluster_nodes\n",
    "                ]\n",
    "\n",
    "        # Nommer le fichier de sortie basé sur le fichier du cluster\n",
    "        cluster_label = os.path.splitext(cluster_file)[0]\n",
    "        output_path = os.path.join(output_dir, f\"{cluster_label}_links.json\")\n",
    "\n",
    "        # Sauvegarder les liens filtrés dans un fichier JSON\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cluster_links, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Fichier de liens créé pour le cluster : {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph de similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarity_graph(content, similarity_matrix, output_file):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Ajouter les nœuds\n",
    "    for page in content.keys():\n",
    "        G.add_node(page)\n",
    "\n",
    "    # Ajouter les arêtes pondérées par les similarités textuelles\n",
    "    pages = list(content.keys())\n",
    "    for i in range(len(pages)):\n",
    "        for j in range(i + 1, len(pages)):\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            if similarity > 0:  # Ajouter une arête seulement si la similarité est positive\n",
    "                G.add_edge(pages[i], pages[j], weight=similarity)\n",
    "\n",
    "    # Sauvegarder le graphe en format GML\n",
    "    nx.write_gml(G, output_file)\n",
    "    print(f\"Graphe de similarité créé : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisis ce que tu veux lancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chemin du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    content_path = 'contentB.json'\n",
    "    links_path = 'linksB.json'\n",
    "    content, links = load_data(content_path, links_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entrainer le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_doc2vec_model(content, vector_size=100, window=5, epochs=50)\n",
    "tfidf_matrix = train_tfidf_model(content)\n",
    "doc_vectors = ([model.dv[i] for i in range(len(content))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases content un/des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_find = ['diversity', 'inclusion', 'equity']\n",
    "sentences = find_sentences_with_words(content, words_to_find)\n",
    "\n",
    "with open('output/sentences_with_words.txt', 'w', encoding='utf-8') as file:\n",
    "    pass\n",
    "\n",
    "with open('output/sentences_with_words.txt', 'w', encoding='utf-8') as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "result = word_sentiment_analysis(content, [\"performance\", \"inclusion\", \"communication\"])\n",
    "\n",
    "print(f\"Analyse de sentiment pour le mot '{result['word']}':\")\n",
    "print(f\"Scores moyens : {result['sentiment_scores']}\")\n",
    "print(f\"Sentiment global : {result['overall_sentiment']}\")\n",
    "print(\"\\nExemples de phrases contenant le mot :\")\n",
    "for sentence in result[\"sentences_with_word\"][:5]:  # Afficher jusqu'à 5 phrases\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse de sent. 1 page\n",
    "page_name = \"Page 1\"\n",
    "analyze_page_sentiment(page_name, content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#analyse de sent. corpus\n",
    "corpus = \"\"\n",
    "corpus_sentiment_analysis = analyze_corpus_sentiment(corpus)\n",
    "print(corpus_sentiment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'cleaned_data'\n",
    "clean_and_save_tokens(content, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'contentB.json'  # Remplacez par votre fichier JSON\n",
    "keyword = \"white\"         # Mot-clé à analyser\n",
    "find_cooccurrences(corpus_path, keyword, min_freq=47, window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'contentB.json'  # Remplacez par votre fichier JSON\n",
    "keyword = \"white\"         # Mot-clé à analyser\n",
    "find_trigram_cooccurrences(corpus_path, keyword, min_freq=10, window_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = show_top_tokens(content)\n",
    "print(\"Top Tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Tokens Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'output/token_table.csv'\n",
    "generate_token_table(content_path, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similiratité entre 2 docs /avec 1 doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité entre doc 0 et doc 1 : 0.774471\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarité entre doc 0 et doc 1 :\", calculate_similarity(model, '0', '1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents sauvegardés dans outputidm/Cooperative.txt\n",
      "Documents similaires à 'Cooperative':\n",
      "- International Cooperative Alliance (similarité: 0.5882)\n",
      "- Mondragon Cooperative Corporation (similarité: 0.5837)\n",
      "- Mondragón Cooperative Corporation (similarité: 0.5837)\n",
      "- Problem solving (similarité: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "title = \"Cooperative\"\n",
    "similar_docs_tfidf = find_similar_docs_tfidf(content, tfidf_matrix, title)\n",
    "print(f\"Documents similaires à '{title}':\")\n",
    "for doc, similarity in similar_docs_tfidf:\n",
    "    print(f\"- {doc} (similarité: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = StandardScaler().fit_transform(doc_vectors)\n",
    "tfidf_vectors = StandardScaler().fit_transform(tfidf_matrix.toarray())\n",
    "combined_vectors = np.hstack((doc_vectors, tfidf_matrix.toarray()))\n",
    "\n",
    "optimal_k = find_optimal_clusters(combined_vectors, max_clusters=9, min_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, similarity_matrix = cluster_documents(content, optimal_k)\n",
    "save_clusters_individually(clusters, \"contentB.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dir = \"output/clusters\"\n",
    "original_links_path = \"linksB.json\"\n",
    "output_dir = \"cluster_links\"\n",
    "\n",
    "create_links_per_cluster(clusters_dir, original_links_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_label, docs in clusters.items():\n",
    "    print(f\"Cluster {cluster_label} contains {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON to GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = \"linksB.json\"\n",
    "output_file = \"outputgml/graph.gml\"\n",
    "json_to_gml(links, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file1 = \"outputgml/tokens_graph.gml\"\n",
    "output_file2 = \"outputgml/clusters_graph.gml\"\n",
    "tokens_file = \"output/cleaned_data.json\"\n",
    "clusters_file = \"output/clusters.json\"\n",
    "json_to_gml(tokens_file, output_file1)\n",
    "json_to_gml(clusters_file, output_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doesn't work\n",
    "output_file = \"outputgml/similarity_graph.gml\"\n",
    "generate_similarity_graph(content, similarity_matrix, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = find_optimal_clusters(combined_vectors, max_clusters=14, min_clusters=4)\n",
    "for i in range(0,num_clusters):\n",
    "    json_to_gml(f\"cluster_links/cluster_{i}_links.json\", f\"outputgml/cluster_{i}.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A supprimer par la suite, juste j'ai essayé un truc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Retirer les chiffres\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Supprimer les espaces multiples\n",
    "    return text.strip()\n",
    "\n",
    "# Appliquer sur le contenu\n",
    "content = {key: preprocess_text(value) for key, value in content.items()}\n",
    "def tf(content):\n",
    "    # Créer le vecteur TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(list(content.values()))\n",
    "    \n",
    "    # Extraire les termes et leurs spécificités (IDF)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    idf_values = vectorizer.idf_\n",
    "    \n",
    "    # Afficher la spécificité des mots\n",
    "    print(\"Spécificité (IDF) des mots :\")\n",
    "    for term, idf in zip(terms, idf_values):\n",
    "        print(f\"{term}: {idf:.2f}\")\n",
    "    \n",
    "    return X_tfidf  # Retourne la matrice TF-IDF pour d'autres usages\n",
    "\n",
    "\n",
    "tf(content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
