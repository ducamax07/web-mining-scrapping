{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appuie pour faire tourner les fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modules à installer se trouvent dans requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import nltk \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "STOP_WORDS.update(['employee', 'organization', 'work', 'job', 'company', \"'s\"])\n",
    "STEMMER = nltk.stem.SnowballStemmer('english')\n",
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du fichier de contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content_path, links_path):\n",
    "    if not os.path.exists(content_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{content_path}' est introuvable.\")\n",
    "    with open(content_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    if not os.path.exists(links_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{links_path}' est introuvable.\")\n",
    "    with open(links_path, 'r', encoding='utf-8') as file:\n",
    "        links = json.load(file)\n",
    "    \n",
    "    return content, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "def save_to_file(data, filename):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        np.savetxt(f'output/{filename}.txt', data, fmt='%.4f')\n",
    "    elif isinstance(data, dict):\n",
    "        with open(f'output/{filename}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    elif isinstance(data, list):\n",
    "        with open(f'output/{filename}.txt', 'w', encoding='utf-8') as f:\n",
    "            for line in data:\n",
    "                f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON TO GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_gml (links_path, output_file):\n",
    "    with open(links_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        links_path = json.load(file)\n",
    "    nodes = set()\n",
    "    edges = []\n",
    "    # Parcourir les liens et extraire les nœuds et les connexions\n",
    "    for source, targets in links_path.items():\n",
    "        nodes.add(source)\n",
    "        for target in targets:\n",
    "            nodes.add(target)\n",
    "            edges.append((source, target))\n",
    "\n",
    "    # Étape 3 : Assigner des ID aux nœuds (sans utiliser enumerate)\n",
    "    node_id = {}\n",
    "    current_id = 0\n",
    "    for node in nodes:\n",
    "        node_id[node] = current_id\n",
    "        current_id += 1\n",
    "\n",
    "    # Étape 4 : Écrire le fichier GML\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"graph\\n\")\n",
    "        file.write(\"[\\n\")\n",
    "        file.write(\"  directed 1\\n\")  # Graph orienté\n",
    "\n",
    "        # Ajouter les nœuds\n",
    "        for node, id in node_id.items():\n",
    "            file.write(\"  node\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    id {id}\\n\")\n",
    "            file.write(f\"    label \\\"{node}\\\"\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        # Ajouter les arêtes\n",
    "        for source, target in edges:\n",
    "            file.write(\"  edge\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    source {node_id[source]}\\n\")\n",
    "            file.write(f\"    target {node_id[target]}\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        file.write(\"]\\n\")\n",
    "\n",
    "    print(f\"Fichier GML créé : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement du contenu texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [STEMMER.stem(token) for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder tous les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_tokens(content, output_file):\n",
    "    cleaned_data = {}\n",
    "\n",
    "    for page_title, page_content in content.items():\n",
    "        tokens = preprocess_text(page_content)\n",
    "        cleaned_data[page_title] = tokens\n",
    "\n",
    "    save_to_file(cleaned_data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement de texte spécifique à SIA, pour éviter de supprimer les \"not\" et autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_SIA(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    all_tokens = []\n",
    "    for content in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(content))\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement des modèles tfidfj et doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Entraînement du modèle Doc2Vec\n",
    "def train_doc2vec_model(documents, vector_size=100, window=1, epochs=20):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "    model = Doc2Vec(tagged_data, vector_size=vector_size, window=2, min_count=1, workers=4, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "# 2. Entraînement du modèle TF-IDF\n",
    "def train_tfidf_model(documents):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différents modèles de text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouver les phrases comprenant un ou des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_with_words(corpus, words):\n",
    "    sentences_with_words = []\n",
    "    for content in corpus.values():\n",
    "        sentences = sent_tokenize(content)\n",
    "        for sentence in sentences:\n",
    "            if all(word in sentence.lower() for word in words):\n",
    "                sentences_with_words.append(sentence)\n",
    "    return sentences_with_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_sentiment(sentences):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"compound\": 0}\n",
    "    for sentence in sentences:\n",
    "        scores = sia.polarity_scores(sentence)\n",
    "        sentiment_scores[\"positive\"] += scores[\"pos\"]\n",
    "        sentiment_scores[\"negative\"] += scores[\"neg\"]\n",
    "        sentiment_scores[\"neutral\"] += scores[\"neu\"]\n",
    "        sentiment_scores[\"compound\"] += scores[\"compound\"]\n",
    "\n",
    "    # Moyenne des scores\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences > 0:\n",
    "        sentiment_scores = {key: value / num_sentences for key, value in sentiment_scores.items()}\n",
    "\n",
    "    # Déterminer le sentiment principal\n",
    "    if sentiment_scores[\"positive\"] > sentiment_scores[\"negative\"] and sentiment_scores[\"positive\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment = \"Positif\"\n",
    "    elif sentiment_scores[\"negative\"] > sentiment_scores[\"positive\"] and sentiment_scores[\"negative\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment = \"Négatif\"\n",
    "    else:\n",
    "        sentiment = \"Neutre\"\n",
    "\n",
    "    return sentiment_scores, sentiment\n",
    "\n",
    "def word_sentiment_analysis(content, word):\n",
    "\n",
    "    # Trouver les phrases contenant le mot\n",
    "    sentences_with_word = find_sentences_with_words(content, word)\n",
    "    if not sentences_with_word:\n",
    "        return f\"Le mot '{word}' n'apparaît pas dans le corpus.\"\n",
    "\n",
    "    # Analyser le sentiment des phrases\n",
    "    sentiment_scores, sentiment = analyze_word_sentiment(sentences_with_word)\n",
    "\n",
    "    return {\n",
    "        \"word\": word,\n",
    "        \"sentences_with_word\": sentences_with_word,\n",
    "        \"sentiment_scores\": sentiment_scores,\n",
    "        \"overall_sentiment\": sentiment\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bigrams_doc2vec(corpus, top_n=10):\n",
    "    # Tokenize the corpus\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    \n",
    "    # Create bigrams\n",
    "    bigrams = list(nltk.bigrams(all_tokens))\n",
    "    \n",
    "    # Prepare the bigrams for Doc2Vec\n",
    "    bigram_documents = [TaggedDocument(words=[w1, w2], tags=[i]) for i, (w1, w2) in enumerate(bigrams)]\n",
    "    \n",
    "    # Train the Doc2Vec model\n",
    "    model = Doc2Vec(bigram_documents, vector_size=100, window=2, min_count=1, workers=4, epochs=40)\n",
    "    \n",
    "    # Get the most similar bigrams\n",
    "    bigram_vectors = [model.dv[i] for i in range(len(bigrams))]\n",
    "    similarity_matrix = cosine_similarity(bigram_vectors)\n",
    "    \n",
    "    # Find the top N bigrams based on similarity\n",
    "    top_bigrams = []\n",
    "    for i in range(len(bigrams)):\n",
    "        similar_indices = similarity_matrix[i].argsort()[-top_n:][::-1]\n",
    "        for idx in similar_indices:\n",
    "            if idx != i:\n",
    "                top_bigrams.append((bigrams[i], bigrams[idx], similarity_matrix[i][idx]))\n",
    "    \n",
    "    # Sort the bigrams by similarity score\n",
    "    top_bigrams = sorted(top_bigrams, key=lambda x: x[2], reverse=True)[:top_n]\n",
    "    \n",
    "    return top_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_tokens(corpus, top_n=20):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    return freq_dist.most_common(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(corpus, output_filename='wordcloud.png'):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    text = ' '.join(all_tokens)\n",
    "    wordcloud = WordCloud(background_color='white', stopwords=STOP_WORDS, max_words=30, min_font_size=10).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'output/{output_filename}', format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tableau des tops tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_table(json_path, output_csv):\n",
    "    STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "    STEMMER = SnowballStemmer('english')\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    token_counts = {}  # Dictionnaire pour les occurrences\n",
    "    document_counts = defaultdict(int)  # Nombre de documents contenant chaque token\n",
    "    word_map = defaultdict(set)  # Mots originaux associés aux tokens stemmés\n",
    "\n",
    "\n",
    "    for text in content.values():\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stemmed_tokens = [STEMMER.stem(token) for token in tokens if token.isalnum() and token not in STOP_WORDS]\n",
    "        #Compter les occurrences globales\n",
    "        for token in stemmed_tokens:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "        #Compter les documents contenant chaque token (uniquement une fois par document)\n",
    "        unique_tokens = set(stemmed_tokens)\n",
    "        for token in unique_tokens:\n",
    "            document_counts[token] += 1\n",
    "\n",
    "        #Mapper les mots originaux aux tokens stemmés\n",
    "        for word in tokens:\n",
    "            if word.isalnum() and word not in STOP_WORDS:\n",
    "                stemmed = STEMMER.stem(word)\n",
    "                word_map[stemmed].add(word)\n",
    "\n",
    "    # Construire et sauvegarder le tableau en une étape\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Écrire l'en-tête\n",
    "        writer.writerow([\"Token (stemmatisé)\", \"Mots associés\", \"Occurrences\", \"Articles concernés\"])\n",
    "        \n",
    "        # Écrire les données\n",
    "        for token, freq in token_counts.items():\n",
    "            writer.writerow([\n",
    "                token,\n",
    "                ', '.join(word_map[token]),  # Convertir les mots associés en chaîne de caractères\n",
    "                freq,\n",
    "                document_counts[token]\n",
    "            ])\n",
    "    print(f\"Tableau sauvegardé dans {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de similarité entre 2 documents au choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(model, doc_id1, doc_id2):\n",
    "    vec1 = model.dv[doc_id1]\n",
    "    vec2 = model.dv[doc_id2]\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche de documents similaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_docs(model, doc_id, top_n=5):\n",
    "    return model.dv.most_similar(str(doc_id), topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice de similarité par cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(content, model):\n",
    "    # Obtenir les vecteurs des documents\n",
    "    doc_vectors = np.array([model.dv[i] for i in range(len(content))])  # Convertir en matrice NumPy\n",
    "\n",
    "    # Calcul de la matrice de similarité par le cosinus\n",
    "    similarity_matrix = cosine_similarity(doc_vectors)\n",
    "\n",
    "    # Sauvegarder la matrice de similarité dans un fichier\n",
    "    save_to_file(similarity_matrix, 'similarity_matrix.json')  # Sauvegarde en JSON\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre optimal de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(vectors, max_clusters=10, min_clusters=2):\n",
    "    scores = []\n",
    "    for k in range(min_clusters, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        labels = kmeans.fit_predict(vectors)\n",
    "        scores.append(silhouette_score(vectors, labels))\n",
    "\n",
    "    return np.argmax(scores) + min_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_documents(content, optimal_k):\n",
    "\n",
    "    # Préparer les documents\n",
    "    documents = list(content.values())\n",
    "\n",
    "    # Entraîner le modèle Doc2Vec\n",
    "    model = train_doc2vec_model(documents, vector_size=100, window=2, epochs=40)\n",
    "\n",
    "    # Obtenir les vecteurs des documents\n",
    "    doc_vectors = [model.dv[i] for i in range(len(content))]\n",
    "\n",
    "    # Appliquer KMeans\n",
    "    kmeans = KMeans(n_clusters=optimal_k, max_iter=100, n_init=10, random_state=42)\n",
    "    kmeans.fit(doc_vectors)\n",
    "\n",
    "    clusters = {}\n",
    "    for i, doc in enumerate(doc_vectors):\n",
    "        cluster_label = int(kmeans.labels_[i])\n",
    "        document_key = list(content.keys())[i]  # Utilisation de la clé du dictionnaire\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = []\n",
    "        clusters[cluster_label].append(document_key)\n",
    "\n",
    "    # Sauvegarder les clusters\n",
    "    save_to_file(clusters, 'clusters')\n",
    "    \n",
    "    # Créer un fichier texte avec les titres des documents regroupés par clusters\n",
    "    with open('output/clusters_titles.txt', 'w', encoding='utf-8') as f:\n",
    "        for cluster_label, docs in clusters.items():\n",
    "            f.write(f\"Cluster {cluster_label}:\\n\")\n",
    "            for doc in docs:\n",
    "                f.write(f\"- {doc}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Calcul et sauvegarde de la matrice de similarité\n",
    "    similarity_matrix = calculate_cosine_similarity(content, model)\n",
    "\n",
    "    return clusters, similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde les titres complets des documents dans chaque cluster dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clusters_individually(clusters, content_path):\n",
    "    # Charger le contenu à partir du fichier JSON\n",
    "    with open(content_path, 'r', encoding='utf-8') as f:\n",
    "        content = json.load(f)\n",
    "\n",
    "    # Créer un dossier pour stocker les fichiers des clusters\n",
    "    os.makedirs('output/clusters', exist_ok=True)\n",
    "    \n",
    "    # Sauvegarder chaque cluster dans un fichier JSON séparé\n",
    "    for cluster_label, docs in clusters.items():\n",
    "        cluster_content = {doc: content[doc] for doc in docs}  # Documents de ce cluster\n",
    "        output_file = f'output/clusters/cluster_{cluster_label}.json'  # Nom du fichier JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cluster_content, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Cluster {cluster_label} sauvegardé dans {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links par clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_links_per_cluster(clusters_dir, original_links_path, output_dir):\n",
    "    # Charger les liens originaux\n",
    "    with open(original_links_path, 'r', encoding='utf-8') as f:\n",
    "        original_links = json.load(f)\n",
    "\n",
    "    # Créer un répertoire pour stocker les nouveaux fichiers\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Parcourir tous les fichiers de clusters dans le répertoire spécifié\n",
    "    for cluster_file in os.listdir(clusters_dir):\n",
    "        cluster_path = os.path.join(clusters_dir, cluster_file)\n",
    "\n",
    "        # Charger le fichier du cluster\n",
    "        with open(cluster_path, 'r', encoding='utf-8') as f:\n",
    "            cluster_nodes = json.load(f)\n",
    "\n",
    "        # Préparer un dictionnaire pour les liens de ce cluster\n",
    "        cluster_links = {}\n",
    "\n",
    "        # Parcourir les nœuds du cluster et ajouter leurs liens\n",
    "        for node in cluster_nodes:\n",
    "            if node in original_links:\n",
    "                cluster_links[node] = [\n",
    "                    target for target in original_links[node] if target in cluster_nodes\n",
    "                ]\n",
    "\n",
    "        # Nommer le fichier de sortie basé sur le fichier du cluster\n",
    "        cluster_label = os.path.splitext(cluster_file)[0]\n",
    "        output_path = os.path.join(output_dir, f\"{cluster_label}_links.json\")\n",
    "\n",
    "        # Sauvegarder les liens filtrés dans un fichier JSON\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cluster_links, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Fichier de liens créé pour le cluster : {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph de similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarity_graph(content, similarity_matrix, output_file):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Ajouter les nœuds\n",
    "    for page in content.keys():\n",
    "        G.add_node(page)\n",
    "\n",
    "    # Ajouter les arêtes pondérées par les similarités textuelles\n",
    "    pages = list(content.keys())\n",
    "    for i in range(len(pages)):\n",
    "        for j in range(i + 1, len(pages)):\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            if similarity > 0:  # Ajouter une arête seulement si la similarité est positive\n",
    "                G.add_edge(pages[i], pages[j], weight=similarity)\n",
    "\n",
    "    # Sauvegarder le graphe en format GML\n",
    "    nx.write_gml(G, output_file)\n",
    "    print(f\"Graphe de similarité créé : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisis ce que tu veux lancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chemin du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    content_path = 'content3.json'\n",
    "    links_path = 'links3.json'\n",
    "    content, links = load_data(content_path, links_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainer les modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_doc2vec_model(content, vector_size=100, window=2, epochs=40)\n",
    "tfidf_matrix = train_tfidf_model(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases content un/des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_find = ['profit', 'growth']\n",
    "sentences = find_sentences_with_words(content, words_to_find)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"profit\"  \n",
    "result = word_sentiment_analysis(content, word)\n",
    "\n",
    "print(f\"Analyse de sentiment pour le mot '{result['word']}':\")\n",
    "print(f\"Scores moyens : {result['sentiment_scores']}\")\n",
    "print(f\"Sentiment global : {result['overall_sentiment']}\")\n",
    "print(\"\\nExemples de phrases contenant le mot :\")\n",
    "for sentence in result[\"sentences_with_word\"][:5]:  # Afficher jusqu'à 5 phrases\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'cleaned_data'\n",
    "clean_and_save_tokens(content, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = get_top_bigrams(content)\n",
    "print(\"Top Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = show_top_tokens(content)\n",
    "print(\"Top Tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Tokens Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'output/token_table.csv'\n",
    "generate_token_table(content_path, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarité entre 2 doc/ avec 1 doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarité entre doc 0 et doc 1 :\", calculate_similarity(model, '0', '1'))\n",
    "print(\"Documents similaires à doc 0 :\", find_similar_docs(model, '0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = StandardScaler().fit_transform(doc_vectors)\n",
    "tfidf_vectors = StandardScaler().fit_transform(tfidf_matrix.toarray())\n",
    "combined_vectors = np.hstack((doc_vectors, tfidf_matrix.toarray()))\n",
    "\n",
    "optimal_k = find_optimal_clusters(combined_vectors, max_clusters=10, min_clusters=3)\n",
    "print(f\"Nombre optimal de clusters : {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, similarity_matrix = cluster_documents(content, optimal_k)\n",
    "save_clusters_individually(clusters, \"content3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dir = \"output/clusters\"\n",
    "original_links_path = \"links3.json\"\n",
    "output_dir = \"cluster_links\"\n",
    "\n",
    "create_links_per_cluster(clusters_dir, original_links_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON to GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = \"links3.json\"\n",
    "output_file = \"outputgml/graph.gml\"\n",
    "json_to_gml(links, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file1 = \"outputgml/tokens_graph.gml\"\n",
    "output_file2 = \"outputgml/clusters_graph.gml\"\n",
    "tokens_file = \"output/cleaned_data.json\"\n",
    "clusters_file = \"output/clusters.json\"\n",
    "json_to_gml(tokens_file, output_file1)\n",
    "json_to_gml(clusters_file, output_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,optimal_k):\n",
    "    json_to_gml(f\"cluster_links/cluster_{i}_links.json\", f\"outputgml/cluster_{i}.gml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
