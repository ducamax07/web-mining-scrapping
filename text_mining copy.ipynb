{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appuie pour faire tourner les fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modules à installer se trouvent dans requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ducar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ducar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ducar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ducar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "STOP_WORDS.update(['employee', 'organization', 'work', 'job', 'company', \"'s\"])\n",
    "STEMMER = nltk.stem.SnowballStemmer('english')\n",
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du fichier de contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content_path, links_path):\n",
    "    if not os.path.exists(content_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{content_path}' est introuvable.\")\n",
    "    with open(content_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    if not os.path.exists(links_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{links_path}' est introuvable.\")\n",
    "    with open(links_path, 'r', encoding='utf-8') as file:\n",
    "        links = json.load(file)\n",
    "    \n",
    "    return content, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "def save_to_file(data, filename):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        np.savetxt(f'output/{filename}.txt', data, fmt='%.4f')\n",
    "    elif isinstance(data, dict):\n",
    "        with open(f'output/{filename}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    elif isinstance(data, list):\n",
    "        with open(f'output/{filename}.txt', 'w', encoding='utf-8') as f:\n",
    "            for line in data:\n",
    "                f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON TO GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_gml (links_path, output_file):\n",
    "    \n",
    "    nodes = set()\n",
    "    edges = []\n",
    "\n",
    "    # Parcourir les liens et extraire les nœuds et les connexions\n",
    "    for source, targets in links_path.items():\n",
    "        nodes.add(source)\n",
    "        for target in targets:\n",
    "            nodes.add(target)\n",
    "            edges.append((source, target))\n",
    "\n",
    "    # Étape 3 : Assigner des ID aux nœuds (sans utiliser enumerate)\n",
    "    node_id = {}\n",
    "    current_id = 0\n",
    "    for node in nodes:\n",
    "        node_id[node] = current_id\n",
    "        current_id += 1\n",
    "\n",
    "    # Étape 4 : Écrire le fichier GML\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"graph\\n\")\n",
    "        file.write(\"[\\n\")\n",
    "        file.write(\"  directed 1\\n\")  # Graph orienté\n",
    "\n",
    "        # Ajouter les nœuds\n",
    "        for node, id in node_id.items():\n",
    "            file.write(\"  node\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    id {id}\\n\")\n",
    "            file.write(f\"    label \\\"{node}\\\"\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        # Ajouter les arêtes\n",
    "        for source, target in edges:\n",
    "            file.write(\"  edge\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    source {node_id[source]}\\n\")\n",
    "            file.write(f\"    target {node_id[target]}\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        file.write(\"]\\n\")\n",
    "\n",
    "    print(f\"Fichier GML créé : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement du contenu texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [STEMMER.stem(token) for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder tous les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_tokens(content, output_file):\n",
    "    cleaned_data = {}\n",
    "\n",
    "    for page_title, page_content in content.items():\n",
    "        tokens = preprocess_text(page_content)\n",
    "        cleaned_data[page_title] = tokens\n",
    "\n",
    "    save_to_file(cleaned_data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement de texte spécifique à SIA, pour éviter de supprimer les \"not\" et autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_SIA(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    all_tokens = []\n",
    "    for content in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(content))\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différents modèles de text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouver les phrases comprenant un ou des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_with_words(corpus, words):\n",
    "    sentences_with_words = []\n",
    "    for content in corpus.values():\n",
    "        sentences = sent_tokenize(content)\n",
    "        for sentence in sentences:\n",
    "            if all(word in sentence.lower() for word in words):\n",
    "                sentences_with_words.append(sentence)\n",
    "    return sentences_with_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_sentiment(sentences):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"compound\": 0}\n",
    "    for sentence in sentences:\n",
    "        scores = sia.polarity_scores(sentence)\n",
    "        sentiment_scores[\"positive\"] += scores[\"pos\"]\n",
    "        sentiment_scores[\"negative\"] += scores[\"neg\"]\n",
    "        sentiment_scores[\"neutral\"] += scores[\"neu\"]\n",
    "        sentiment_scores[\"compound\"] += scores[\"compound\"]\n",
    "\n",
    "    # Moyenne des scores\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences > 0:\n",
    "        sentiment_scores = {key: value / num_sentences for key, value in sentiment_scores.items()}\n",
    "\n",
    "    # Déterminer le sentiment principal\n",
    "    if sentiment_scores[\"positive\"] > sentiment_scores[\"negative\"] and sentiment_scores[\"positive\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment = \"Positif\"\n",
    "    elif sentiment_scores[\"negative\"] > sentiment_scores[\"positive\"] and sentiment_scores[\"negative\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment = \"Négatif\"\n",
    "    else:\n",
    "        sentiment = \"Neutre\"\n",
    "\n",
    "    return sentiment_scores, sentiment\n",
    "\n",
    "def word_sentiment_analysis(content, word):\n",
    "\n",
    "    # Trouver les phrases contenant le mot\n",
    "    sentences_with_word = find_sentences_with_words(content, word)\n",
    "    if not sentences_with_word:\n",
    "        return f\"Le mot '{word}' n'apparaît pas dans le corpus.\"\n",
    "\n",
    "    # Analyser le sentiment des phrases\n",
    "    sentiment_scores, sentiment = analyze_word_sentiment(sentences_with_word)\n",
    "\n",
    "    return {\n",
    "        \"word\": word,\n",
    "        \"sentences_with_word\": sentences_with_word,\n",
    "        \"sentiment_scores\": sentiment_scores,\n",
    "        \"overall_sentiment\": sentiment\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bigrams(corpus, freq_filter=5, top_n=10):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "    finder.apply_freq_filter(freq_filter)\n",
    "    return finder.nbest(bigram_measures.likelihood_ratio, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_tokens(corpus, top_n=20):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    return freq_dist.most_common(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(corpus, output_filename='wordcloud.png'):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    text = ' '.join(all_tokens)\n",
    "    wordcloud = WordCloud(background_color='white', stopwords=STOP_WORDS, max_words=30, min_font_size=10).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'output/{output_filename}', format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les datas dans des fichiers JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice de similarité par cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_cosine_similarity(content):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(list(content.values()))\n",
    "    \n",
    "    # Calcul de la matrice de similarité par le cosinus\n",
    "    similarity_matrix = cosine_similarity(X_tfidf)\n",
    "    save_to_file(similarity_matrix, 'similarity_matrix')\n",
    "    \n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer le nombre optimal de clusters \n",
    "- par la méthode du coude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(content, max_k=10):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(list(content.values()))\n",
    "    distortions = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, max_iter=100, n_init=5, random_state=42)\n",
    "        kmeans.fit(X_tfidf)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    # Crée le répertoire output s'il n'existe pas\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, max_k + 1), distortions, marker='o')\n",
    "    plt.xlabel('Nombre de Clusters')\n",
    "    plt.ylabel('Distorsion')\n",
    "    plt.title('Méthode du Coude')\n",
    "    plt.savefig('output/elbow_method.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Retourne le nombre optimal basé sur l'inflexion\n",
    "    optimal_k = np.diff(distortions).argmin() + 2\n",
    "    return optimal_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_documents(content, num_clusters):\n",
    "    documents = list(content.values())\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_clusters, max_iter=100, n_init=10, random_state=42)\n",
    "    kmeans.fit(X_tfidf)\n",
    "    \n",
    "    clusters = {}\n",
    "    for i, doc in enumerate(documents):\n",
    "        cluster_label = int(kmeans.labels_[i])\n",
    "        document_key = list(content.keys())[i]  # Utilisation de la clé du dictionnaire\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = []\n",
    "        clusters[cluster_label].append(document_key)\n",
    "    \n",
    "    save_to_file(clusters, 'clusters')\n",
    "\n",
    "    # Calcul et sauvegarde de la matrice de similarité\n",
    "    similarity_matrix = calculate_cosine_similarity(content)\n",
    "    \n",
    "    return clusters, similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde les titres complets des documents dans chaque cluster dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_content(clusters, content):\n",
    "    output_lines = []\n",
    "    for cluster_label, docs in clusters.items():\n",
    "        output_lines.append(f'Cluster {cluster_label}:')\n",
    "        for doc in docs:\n",
    "            output_lines.append(f'  - {doc}')\n",
    "        output_lines.append('')\n",
    "\n",
    "    save_to_file(output_lines, 'cluster_titles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisis ce que tu veux lancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chemin du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    content_path = 'content3.json'\n",
    "    links_path = 'links3.json'\n",
    "    content, links = load_data(content_path, links_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases content un/des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_find = ['profit', 'growth']\n",
    "sentences = find_sentences_with_words(content, words_to_find)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"profit\"  \n",
    "result = word_sentiment_analysis(content, word)\n",
    "\n",
    "print(f\"Analyse de sentiment pour le mot '{result['word']}':\")\n",
    "print(f\"Scores moyens : {result['sentiment_scores']}\")\n",
    "print(f\"Sentiment global : {result['overall_sentiment']}\")\n",
    "print(\"\\nExemples de phrases contenant le mot :\")\n",
    "for sentence in result[\"sentences_with_word\"][:5]:  # Afficher jusqu'à 5 phrases\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'cleaned_data'\n",
    "clean_and_save_tokens(content, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = get_top_bigrams(content)\n",
    "print(\"Top Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = show_top_tokens(content)\n",
    "print(\"Top Tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = find_optimal_clusters(content, max_k=10)\n",
    "print(f'Nombre optimal de clusters : {num_clusters}')\n",
    "clusters, similarity_matrix = cluster_documents(content, num_clusters)\n",
    "display_cluster_content(clusters, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON to GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"graph.gml\"\n",
    "json_to_gml(links, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier GML créé : tokens_graph.gml\n"
     ]
    }
   ],
   "source": [
    "output_file = \"tokens_graph.gml\"\n",
    "tokens_file = \"output/cleaned_data.json\"\n",
    "clusters_file = \"output/clusters.json\"\n",
    "tokens, clusters = load_data(tokens_file , clusters_file)\n",
    "json_to_gml(tokens, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
