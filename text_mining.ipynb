{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appuie pour faire tourner les fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.11.2 nécessitent le package ipykernel.\n",
      "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
      "\u001b[1;31mCommande : '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "STOP_WORDS.update(['employee', 'organization', 'work', 'job', 'company', \"'s\"])\n",
    "STEMMER = nltk.stem.SnowballStemmer('english')\n",
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du fichier de contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content_path, links_path):\n",
    "    if not os.path.exists(content_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{content_path}' est introuvable.\")\n",
    "    with open(content_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    if not os.path.exists(links_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{links_path}' est introuvable.\")\n",
    "    with open(links_path, 'r', encoding='utf-8') as file:\n",
    "        links = json.load(file)\n",
    "    \n",
    "    return content, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_content(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement du contenu texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [STEMMER.stem(token) for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    all_tokens = []\n",
    "    for content in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(content))\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différents modèles de text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_sentiment(word1, word2, corpus):\n",
    "    positive, negative, neutral = 0, 0, 0\n",
    "    for content in corpus.values():\n",
    "        sentences = sent_tokenize(content)\n",
    "        for sentence in sentences:\n",
    "            tokens = preprocess_text(sentence)\n",
    "            if word1 in tokens and word2 in tokens:\n",
    "                sentiment = SIA.polarity_scores(sentence)\n",
    "                if sentiment['compound'] > 0.05:\n",
    "                    positive += 1\n",
    "                elif sentiment['compound'] < -0.05:\n",
    "                    negative += 1\n",
    "                else:\n",
    "                    neutral += 1\n",
    "    total = positive + negative + neutral\n",
    "    return {'Words': (word1, word2),'Positive': positive,'Negative': negative,'Neutral': neutral,'Total': total,\n",
    "            'Positive Ratio': f\"{(positive / total * 100):.0f}%\" if total else '0%','Negative Ratio': f\"{(negative / total * 100):.0f}%\" if total else '0%'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bigrams(corpus, freq_filter=5, top_n=10):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "    finder.apply_freq_filter(freq_filter)\n",
    "    return finder.nbest(bigram_measures.likelihood_ratio, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_tokens(corpus, top_n=20):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    return freq_dist.most_common(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(corpus):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    text = ' '.join(all_tokens)\n",
    "    wordcloud = WordCloud(background_color='white',stopwords=STOP_WORDS,max_words=30,min_font_size=10).generate(text)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(content):\n",
    "    \"\"\"\n",
    "    Calcule la matrice de similarité par le cosinus entre tous les documents du contenu.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(list(content.values()))\n",
    "    \n",
    "    # Calcul de la matrice de similarité par le cosinus\n",
    "    similarity_matrix = cosine_similarity(X_tfidf)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def cluster_documents(content, num_clusters=3):\n",
    "    \"\"\"\n",
    "    Fonction qui effectue le clustering des documents et retourne les résultats sous forme de dictionnaire.\n",
    "    \"\"\"\n",
    "    documents = list(content.values())\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Application de l'algorithme KMeans pour le clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, max_iter=100, n_init=1)\n",
    "    kmeans.fit(X_tfidf)\n",
    "    \n",
    "    clusters = {}\n",
    "    for i, doc in enumerate(documents):\n",
    "        cluster_label = int(kmeans.labels_[i])\n",
    "        document_key = list(content.keys())[i]  # Utilisation de la clé du dictionnaire\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = []\n",
    "        clusters[cluster_label].append(document_key)\n",
    "    \n",
    "    # Calcul de la matrice de similarité par le cosinus\n",
    "    similarity_matrix = calculate_cosine_similarity(content)\n",
    "    \n",
    "    return clusters, similarity_matrix\n",
    "\n",
    "def display_cluster_content(clusters, max_docs=3):\n",
    "    \"\"\"\n",
    "    Affiche les titres de 3 documents pour chaque cluster.\n",
    "    \"\"\"\n",
    "    print(\"Titres des documents par cluster (maximum 3 documents par cluster) :\")\n",
    "    for cluster_label, docs in clusters.items():\n",
    "        print(f\"\\nCluster {cluster_label}:\")\n",
    "        # Limiter à 3 documents par cluster\n",
    "        for doc in docs[:max_docs]:\n",
    "            print(f\"  - {doc}\")\n",
    "        if len(docs) > max_docs:\n",
    "            print(\"  ...\")  # Indication qu'il y a plus de documents dans le cluster\n",
    "\n",
    "def display_cluster_info(clusters, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Affiche les informations des clusters et la matrice de similarité.\n",
    "    \"\"\"\n",
    "    # Affichage du nombre de documents par cluster\n",
    "    print(\"Nombre de documents par cluster :\")\n",
    "    for cluster_label, docs in clusters.items():\n",
    "        print(f\"Cluster {cluster_label}: ({len(docs)} documents)\")\n",
    "    \n",
    "    # Affichage de la matrice de similarité\n",
    "    print(\"\\nMatrice de Similarité par le Cosinus :\")\n",
    "    print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisis ce que tu veux lancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chemin du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    file_path = 'content1.json'\n",
    "    corpus = load_data(content_path, links_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result = analyze_word_sentiment('profit', 'growth', corpus)\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = get_top_bigrams(corpus)\n",
    "print(\"Top Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = show_top_tokens(corpus)\n",
    "print(\"Top Tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, similarity_matrix = cluster_documents(content, num_clusters=5)\n",
    "\n",
    "# Affichage des informations des clusters\n",
    "display_cluster_info(clusters, similarity_matrix)\n",
    "\n",
    "# Affichage des titres des documents pour chaque cluster (maximum 3 documents par cluster)\n",
    "display_cluster_content(clusters, max_docs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
