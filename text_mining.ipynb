{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appuie pour faire tourner les fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modules à installer se trouvent dans requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "STOP_WORDS.update(['employee', 'organization', 'work', 'job', 'company', \"'s\"])\n",
    "STEMMER = nltk.stem.SnowballStemmer('english')\n",
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du fichier de contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content_path, links_path):\n",
    "    if not os.path.exists(content_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{content_path}' est introuvable.\")\n",
    "    with open(content_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    if not os.path.exists(links_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{links_path}' est introuvable.\")\n",
    "    with open(links_path, 'r', encoding='utf-8') as file:\n",
    "        links = json.load(file)\n",
    "    \n",
    "    return content, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_content(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement du contenu texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [STEMMER.stem(token) for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    all_tokens = []\n",
    "    for content in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(content))\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour transformer un le fichier links en GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_gml (links_json):\n",
    "    # Charger le fichier JSON\n",
    "    output_file = \"graph.gml\"  # Le fichier GML que vous voulez créer\n",
    "    # Étape 2 : Créer des listes pour les nœuds et les arêtes\n",
    "    nodes = set()\n",
    "    edges = []\n",
    "\n",
    "    # Parcourir les liens et extraire les nœuds et les connexions\n",
    "    for source, targets in links_json.items():\n",
    "        nodes.add(source)\n",
    "        for target in targets:\n",
    "            nodes.add(target)\n",
    "            edges.append((source, target))\n",
    "\n",
    "    # Étape 3 : Assigner des ID aux nœuds (sans utiliser enumerate)\n",
    "    node_id = {}\n",
    "    current_id = 0\n",
    "    for node in nodes:\n",
    "        node_id[node] = current_id\n",
    "        current_id += 1\n",
    "\n",
    "    # Étape 4 : Écrire le fichier GML\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"graph\\n\")\n",
    "        file.write(\"[\\n\")\n",
    "        file.write(\"  directed 1\\n\")  # Graph orienté\n",
    "\n",
    "        # Ajouter les nœuds\n",
    "        for node, id in node_id.items():\n",
    "            file.write(\"  node\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    id {id}\\n\")\n",
    "            file.write(f\"    label \\\"{node}\\\"\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        # Ajouter les arêtes\n",
    "        for source, target in edges:\n",
    "            file.write(\"  edge\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    source {node_id[source]}\\n\")\n",
    "            file.write(f\"    target {node_id[target]}\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "\n",
    "        file.write(\"]\\n\")\n",
    "\n",
    "    print(f\"Fichier GML créé : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différents modèles de text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement de texte spécifique à SIA, pour éviter de supprimer les \"not\" et autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_SIA(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_sentiment(word1, word2, corpus):\n",
    "    positive, negative, neutral = 0, 0, 0\n",
    "    for content in corpus.values():\n",
    "        # Diviser le texte en phrases\n",
    "        sentences = sent_tokenize(content)\n",
    "        for sentence in sentences:\n",
    "            # Prétraiter chaque phrase\n",
    "            tokens = preprocess_SIA(sentence)\n",
    "            # Vérifier si les deux mots sont présents\n",
    "            if word1 in tokens and word2 in tokens:\n",
    "                # Analyse des sentiments\n",
    "                sentiment = SIA.polarity_scores(sentence)\n",
    "                if sentiment['compound'] > 0.02:  # Seuil ajusté pour plus de sensibilité\n",
    "                    positive += 1\n",
    "                elif sentiment['compound'] < -0.02:\n",
    "                    negative += 1\n",
    "                else:\n",
    "                    neutral += 1\n",
    "    # Calculer les ratios\n",
    "    total = positive + negative + neutral\n",
    "    if total > 0:\n",
    "        positive_ratio = f\"{(positive / total * 100):.0f}%\"\n",
    "        negative_ratio = f\"{(negative / total * 100):.0f}%\"\n",
    "    else:\n",
    "        positive_ratio = negative_ratio = \"0%\"\n",
    "\n",
    "    return {\n",
    "        'Words': (word1, word2),\n",
    "        'Positive': positive,\n",
    "        'Negative': negative,\n",
    "        'Neutral': neutral,\n",
    "        'Total': total,\n",
    "        'Positive Ratio': positive_ratio,\n",
    "        'Negative Ratio': negative_ratio\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bigrams(corpus, freq_filter=5, top_n=10):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "    finder.apply_freq_filter(freq_filter)\n",
    "    return finder.nbest(bigram_measures.likelihood_ratio, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_tokens(corpus, top_n=20):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    return freq_dist.most_common(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(corpus):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    text = ' '.join(all_tokens)\n",
    "    wordcloud = WordCloud(background_color='white',stopwords=STOP_WORDS,max_words=30,min_font_size=10).generate(text)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les datas dans des fichiers JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "def save_to_file(data, filename):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        np.savetxt(f'output/{filename}.txt', data, fmt='%.4f')\n",
    "    elif isinstance(data, dict):\n",
    "        with open(f'output/{filename}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    elif isinstance(data, list):\n",
    "        with open(f'output/{filename}.txt', 'w', encoding='utf-8') as f:\n",
    "            for line in data:\n",
    "                f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice de similarité par cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_cosine_similarity(content):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(list(content.values()))\n",
    "    \n",
    "    # Calcul de la matrice de similarité par le cosinus\n",
    "    similarity_matrix = cosine_similarity(X_tfidf)\n",
    "    save_to_file(similarity_matrix, 'similarity_matrix')\n",
    "    \n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer le nombre optimal de clusters \n",
    "- par la méthode du coude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(content, max_k=10):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(list(content.values()))\n",
    "    distortions = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, max_iter=100, n_init=5, random_state=42)\n",
    "        kmeans.fit(X_tfidf)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    # Crée le répertoire output s'il n'existe pas\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, max_k + 1), distortions, marker='o')\n",
    "    plt.xlabel('Nombre de Clusters')\n",
    "    plt.ylabel('Distorsion')\n",
    "    plt.title('Méthode du Coude')\n",
    "    plt.savefig('output/elbow_method.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Retourne le nombre optimal basé sur l'inflexion\n",
    "    optimal_k = np.diff(distortions).argmin() + 2\n",
    "    return optimal_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_documents(content, num_clusters):\n",
    "    documents = list(content.values())\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Application de l'algorithme KMeans pour le clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, max_iter=100, n_init=10, random_state=42)\n",
    "    kmeans.fit(X_tfidf)\n",
    "    \n",
    "    clusters = {}\n",
    "    for i, doc in enumerate(documents):\n",
    "        cluster_label = int(kmeans.labels_[i])\n",
    "        document_key = list(content.keys())[i]  # Utilisation de la clé du dictionnaire\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = []\n",
    "        clusters[cluster_label].append(document_key)\n",
    "    \n",
    "    # Sauvegarder les clusters\n",
    "    save_to_file(clusters, 'clusters')\n",
    "\n",
    "    # Calcul et sauvegarde de la matrice de similarité\n",
    "    similarity_matrix = calculate_cosine_similarity(content)\n",
    "    \n",
    "    return clusters, similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde les titres complets des documents dans chaque cluster dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_content(clusters, content):\n",
    "    output_lines = []\n",
    "    for cluster_label, docs in clusters.items():\n",
    "        output_lines.append(f'Cluster {cluster_label}:')\n",
    "        for doc in docs:\n",
    "            output_lines.append(f'  - {doc}')\n",
    "        output_lines.append('')\n",
    "\n",
    "    save_to_file(output_lines, 'cluster_titles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisis ce que tu veux lancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json links --> GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier GML créé : graph.gml\n"
     ]
    }
   ],
   "source": [
    "json_to_gml(load_data('content3.json', 'links3.json')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chemin du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    content_path = 'content3.json'\n",
    "    links_path = 'links3.json'\n",
    "    content, links = load_data(content_path, links_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result = analyze_word_sentiment('profit', 'growth', content)\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = get_top_bigrams(content)\n",
    "print(\"Top Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = show_top_tokens(content)\n",
    "print(\"Top Tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = find_optimal_clusters(content, max_k=10)\n",
    "print(f'Nombre optimal de clusters : {num_clusters}')\n",
    "\n",
    "# Clusteriser avec le nombre optimal\n",
    "clusters, similarity_matrix = cluster_documents(content, num_clusters)\n",
    "display_cluster_content(clusters, content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
