{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB MINING - Diversity and inclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generals imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import deque, defaultdict\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration NLTK et Wikipedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wikipedia.set_lang(\"en\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparer les stopwords et le stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english'))) + [\"'s\"]\n",
    "stem = nltk.stem.SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [stem.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtention des n tokens les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tokens(content, n=20):\n",
    "    tokens = extract_tokens(content)\n",
    "    token_counts = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        token_counts[token] += 1\n",
    "    sorted_tokens = sorted(token_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    return set(item[0] for item in sorted_tokens[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification de pertinence via les tokens principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_based_on_top_tokens(top_tokens, linked_summary, threshold=5):\n",
    "    linked_tokens = extract_tokens(linked_summary)\n",
    "    common_tokens = [token for token in linked_tokens if token in top_tokens]\n",
    "    return len(common_tokens) >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde et chargement des datas (sous json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    return {}\n",
    "\n",
    "def save_data(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde de liens (sous gml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_links_as_gml(link_storage, gml_file):\n",
    "    graph = nx.DiGraph()\n",
    "    for source, targets in link_storage.items():\n",
    "        for target in targets:\n",
    "            graph.add_edge(source, target)\n",
    "    nx.write_gml(graph, gml_file)\n",
    "    print(f\"Graph saved to {gml_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main fonction - scrapping and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_store_with_recovery(start_page_title, max_depth=5, content_file='content.json', link_file='links.json',\n",
    "                                   visited_file='visited.json', queue_file='queue.json'):\n",
    "    content_storage = load_data(content_file)\n",
    "    link_storage = load_data(link_file)\n",
    "    visited_pages = set(load_data(visited_file))\n",
    "    queue = deque(load_data(queue_file) if os.path.exists(queue_file) else [(start_page_title, 0)])\n",
    "\n",
    "    main_page = wikipedia.WikipediaPage(start_page_title)\n",
    "    main_content = main_page.content\n",
    "    top_tokens = get_top_tokens(main_content, n=20)\n",
    "\n",
    "    while queue:\n",
    "        save_data(queue_file, list(queue))\n",
    "        current_page_title, current_depth = queue.popleft()\n",
    "\n",
    "        if current_depth >= max_depth or current_page_title in visited_pages:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            current_page = wikipedia.WikipediaPage(current_page_title)\n",
    "            visited_pages.add(current_page_title)\n",
    "            save_data(visited_file, list(visited_pages))\n",
    "            content_storage[current_page_title] = current_page.content\n",
    "            save_data(content_file, content_storage)\n",
    "\n",
    "            for link in current_page.links:\n",
    "                if link in visited_pages:\n",
    "                    continue\n",
    "                try:\n",
    "                    linked_page = wikipedia.WikipediaPage(link)\n",
    "                    if is_relevant_based_on_top_tokens(top_tokens, linked_page.summary, threshold=5):\n",
    "                        if current_page_title not in link_storage:\n",
    "                            link_storage[current_page_title] = []\n",
    "                        link_storage[current_page_title].append(link)\n",
    "                        save_data(link_file, link_storage)\n",
    "                        queue.append((linked_page.title, current_depth + 1))\n",
    "                except wikipedia.exceptions.DisambiguationError:\n",
    "                    continue\n",
    "                except wikipedia.exceptions.PageError:\n",
    "                    continue\n",
    "        except wikipedia.exceptions.DisambiguationError:\n",
    "            continue\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            continue\n",
    "\n",
    "    save_data(queue_file, [])\n",
    "    print(\"Scraping completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancement du scrapping et sauvegarde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_and_store_with_recovery(\"Diversity (business)\", max_depth=5)\n",
    "\n",
    "link_storage = load_data('links.json')\n",
    "save_links_as_gml(link_storage, 'graph.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT MINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINKS ANALYSIS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
